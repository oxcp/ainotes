##🧪 什么是 SWE-bench？
SWE-bench（Software Engineering Benchmark）是一个用于评估大型语言模型（LLMs）在真实世界软件开发任务中表现的基准测试。它由 Princeton NLP Lab 和 Weights & Biases 等研究机构联合开发，旨在衡量模型是否能够自动修复真实的 GitHub 问题。

##🔍 核心特点
| 特性             | 描述                                                                                   |
|------------------|----------------------------------------------------------------------------------------|
| 真实任务来源     | 来自 12 个流行开源项目的 GitHub issue 和 pull request。                                 |
| 任务类型         | 包括 bug 修复、功能实现、重构、测试生成等。                                            |
| 评估方式         | 模型生成代码补丁后，系统会运行项目的单元测试来验证是否成功修复问题。                   |
| 自动化环境       | 每个任务在 Docker 容器中运行，确保环境一致性和可重复性。                               |
| Agentic 支持     | 支持模型以“代理”方式操作代码库（如导航、编辑、测试等）。                              |
##📦 数据集版本
版本	描述
| 版本                    | 描述                                                         |
|-------------------------|--------------------------------------------------------------|
| SWE-bench Full          | 包含 2,294 个任务，覆盖广泛的项目和问题类型。                |
| SWE-bench Verified      | 500 个经过人工验证的任务，确保每个任务都有明确的解决方案。    |
| SWE-bench Lite          | 精简版，适合快速测试和实验。                                 |
| SWE-bench Multimodal    | 包含截图等视觉信息，适用于多模态模型。                        |
| SWE-bench Bash Only     | 专注于 shell 脚本任务。                                      |
##🏆 为什么重要？
SWE-bench 是目前最接近真实开发场景的基准之一。它不仅考察模型的代码生成能力，还评估其在复杂工程环境中的理解、导航、修改和验证能力。这使得它成为衡量“Agentic AI”能力的关键工具。