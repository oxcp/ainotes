SWE-Lancer 是 OpenAI 于 2025 年推出的一个全新基准，旨在评估大型语言模型（LLMs）在真实世界自由职业软件工程任务中的表现。它是继 SWE-Bench 和 SWE-Bench Verified 之后，更贴近现实工程场景的评测框架。

## 什么是 SWE-Lancer？
SWE-Lancer 是一个包含 1,488 个真实 Upwork 自由职业任务 的基准数据集，这些任务总价值超过 100 万美元，涵盖了从简单的 bug 修复到复杂的功能开发，甚至包括技术管理决策。

## SWE-Lancer Diamond 是什么？
- SWE-Lancer Diamond 是该基准的一个公开评估子集，专门用于社区研究和模型评测。它包括：
    - **独立贡献者（IC SWE）任务**：模型需编写代码解决实际问题。
    - **管理者（SWE Manager）任务**：模型需在多个技术实现方案中做出选择，模拟技术领导角色。



## 任务类型与评估方式
| 类型         | 描述                       | 评估方式                         |
|--------------|----------------------------|-----------------------------------|
| IC SWE       | 编写代码修复 bug 或实现功能 | 通过三重验证的端到端测试          |
| SWE Manager  | 选择最佳技术方案            | 与原始工程经理的选择对比          |

## 模型表现（截至 2025 年 7 月）
- 最先进模型（如 GPT-4o）在 SWE-Lancer 上的整体成功率仍低于 50%，显示真实工程任务对模型仍具挑战性。
- 在 SWE-Lancer Diamond 子集上，模型表现更稳定，因为该子集移除了对联网执行的依赖，减少了评估波动。


## 工具与资源
- 提供统一的 Docker 镜像，便于复现和评估
- 数据集与评估脚本已开源：[GitHub - SWE-Lancer Benchmark](https://github.com/SWE-Lancer/Benchmark)


## 研究意义
- 衡量 AI 在真实工程环境中的经济价值；
- 推动对 AI 工程能力、代理安全（agentic safety）和协作能力的研究；
- 为未来的 AI 工程代理（AI software agents）开发提供现实基础。
